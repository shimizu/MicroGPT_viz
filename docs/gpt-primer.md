# GPT入門（`microgpt` を読む前提の最小知識）

この文書は、`docs/microgpt.md` の補助として「GPTとは何か」を短く整理したものです。

## 1. GPTを一言で

GPT（Generative Pre-trained Transformer）は、
「これまでのトークン列を見て、次の1トークンの確率分布を予測する」モデルです。

- 入力: `x1, x2, ..., xt`
- 出力: `xt+1` の候補ごとの確率

生成はこの予測を繰り返すだけです。

## 2. 何を学習しているのか

学習時は正解テキストがあり、各位置で次トークンを当てるタスクを解きます。

- 目的関数: 交差エントロピー損失
- 直感: 正解トークンの確率を上げるように重みを更新

言い換えると、GPTは「大量の次トークン予測問題」を解いて言語パターンを獲得します。

## 3. Transformerの中核

GPTの本体はTransformer decoderブロックの積み重ねです。

1. トークン埋め込み + 位置埋め込み
2. 自己注意（Self-Attention）
3. MLP（全結合の非線形変換）
4. 残差接続 + 正規化

### Self-Attentionの直感

各位置が、過去トークンのどこをどれだけ参照するかを重みで決めます。

- Query: 今ほしい情報
- Key: 各トークンが持つ見出し
- Value: 実際に取り出す情報

`Query` と `Key` の相性で重みを作り、`Value` を重み付き和します。

## 4. なぜ「過去だけ」を見るのか

GPTは自己回帰（autoregressive）モデルです。
次トークン予測で未来情報を見るとチートになるため、訓練時も推論時も「現在位置より未来」は見ません。

`microgpt` では1トークンずつ順に処理しているため、結果的にこの制約を満たしています。

## 5. 学習と推論の違い

- 学習: 正解トークンを使って次を予測（teacher forcing）
- 推論: 自分が出したトークンを次の入力に使う

この差のため、推論では誤りが連鎖することがあります。

## 6. 生成のランダム性（temperature）

出力確率の鋭さを温度で調整します。

- 低温（例: `0.2`）: 確率上位に集中、保守的
- 高温（例: `1.0` 以上）: 多様だが不安定

`microgpt` では `temperature = 0.5` が設定されています。

## 7. パラメータ数と性能

一般に、パラメータ数が多いほど表現力は上がりますが、計算資源とデータが必要です。

`microgpt` は教育用の超小型設定で、実用性能より「仕組みの可視化」を優先しています。

## 8. `microgpt` と実用LLMの違い

- トークン化: 文字単位（実用LLMはサブワード）
- 規模: 1層・小埋め込み（実用は多数層・大次元）
- 実装: スカラー自動微分（実用はテンソル/GPU最適化）
- 学習: 単純ループ（実用は大規模分散訓練）

ただし、次の骨格は共通です。

- 次トークン予測
- Attention
- 逆伝播
- 勾配ベース最適化

## 9. 最低限覚える式（概念用）

- 次トークン確率: `p(xt+1 | x1...xt)`
- Attention重み: `softmax(QK^T / sqrt(d))`
- 損失: `-log p(正解トークン)`

式を暗記する必要はありません。`microgpt` のコードで「どこで計算しているか」を対応づけられれば十分です。

## 10. 次に読むべきファイル

1. `docs/microgpt.md`
2. `docs/microgpt-reading-map.md`
3. `src/microgpt.js`
