# バックプロパゲーション（逆伝播）の仕組み

この文書は、`src/microgpt.js` のコードを題材に、**バックプロパゲーション（逆伝播）** を解説します。
AIプログラミングの経験がない開発者でも、コードと対応させながら理解できることを目指しています。

## 関連ドキュメント

- GPTの概要: [`docs/gpt-primer.md`](./gpt-primer.md)
- Transformerの仕組み: [`docs/transformer-primer.md`](./transformer-primer.md)
- コード読解ガイド: [`docs/microgpt.md`](./microgpt.md)

---

## 1. バックプロパゲーションとは何か

### 一言で

**「予測がどれだけ間違っていたか（損失）を起点に、各パラメータがどれだけ間違いに貢献したかを逆算する手法」** です。

### 通常のプログラムとの違い

通常のプログラムでは、入力を加工して出力を返せば終わりです。
ニューラルネットでは「出力が間違っていたら、中の数値（重み）を自動で修正したい」という要求があります。

そこで必要になるのが **勾配（gradient）** ── 「この重みを少し変えたら、損失はどれだけ変わるか」を表す数値です。
バックプロパゲーションは、この勾配を出力側から入力側へ逆向きに効率よく計算するアルゴリズムです。

### 身近なたとえ

レシピ（パラメータ）を調整して料理（出力）の味（損失）を改善する状況を想像してください。

1. 料理を作る（**フォワードパス** = 前方計算）
2. 味見して「辛すぎる」と評価する（**損失の計算**）
3. 「辛さの原因は何か？」と材料を逆にたどる（**バックプロパゲーション**）
4. 原因の材料を減らす（**パラメータ更新**）

バックプロパゲーションは **ステップ3** にあたります。

---

## 2. `Value` クラス — 自動微分の仕組み

バックプロパゲーションの土台は `Value` クラス（`src/microgpt.js:75`）です。

### 2.1 普通の数値との違い

JavaScript の `number` はただの値です。`Value` はそれに加えて以下を持ちます。

```javascript
class Value {
    constructor(data, children = [], localGrads = []) {
        this.data = data;           // 前方計算の結果値（普通の数値と同じ）
        this.grad = 0;              // ← これが「勾配」。逆伝播で埋まる
        this._children = children;  // この値を生んだ入力ノード（親子関係）
        this._localGrads = localGrads; // 各入力に対する局所的な変化率
    }
}
```

つまり `Value` は **「計算の履歴を自動で記録する数値」** です。

### 2.2 計算グラフの自動構築

`Value` 同士を演算すると、新しい `Value` が生成され、親子関係が記録されます。

```javascript
const a = new Value(3);        // 葉ノード
const b = new Value(2);        // 葉ノード
const c = a.mul(b);            // c.data = 6, children = [a, b]
const d = c.add(new Value(1)); // d.data = 7, children = [c, Value(1)]
```

これで以下のような **計算グラフ**（木構造）が自動的にできます。

```
d = 7
├── c = 6
│   ├── a = 3
│   └── b = 2
└── Value(1)
```

GPTの全計算（埋め込み → Attention → MLP → 損失）が、この仕組みで 1つの巨大な計算グラフになります。

### 2.3 ローカル勾配 — 各演算の「変化率レシピ」

各演算は **「入力が少し変わったら出力がどれだけ変わるか」** を `localGrads` に記録します。
これは微分の基本ルールそのものです。

| 演算 | 数式 | ローカル勾配 | コード位置 |
|------|------|-------------|-----------|
| 加算 `a + b` | ∂(a+b)/∂a = 1, ∂(a+b)/∂b = 1 | `[1, 1]` | `src/microgpt.js:89` |
| 乗算 `a * b` | ∂(ab)/∂a = b, ∂(ab)/∂b = a | `[b.data, a.data]` | `src/microgpt.js:95` |
| 累乗 `a^n` | ∂(a^n)/∂a = n·a^(n-1) | `[n * a^(n-1)]` | `src/microgpt.js:101` |
| 対数 `ln(a)` | ∂ln(a)/∂a = 1/a | `[1/a.data]` | `src/microgpt.js:110` |
| 指数 `exp(a)` | ∂exp(a)/∂a = exp(a) | `[exp(a.data)]` | `src/microgpt.js:115` |
| ReLU `max(0,a)` | a > 0 なら 1、そうでなければ 0 | `[a > 0 ? 1 : 0]` | `src/microgpt.js:121` |

微分の公式を覚える必要はありません。ポイントは **「各演算が変化率を自動で記録する」** ことです。

---

## 3. `backward()` — 逆伝播の実行

### 3.1 連鎖律（Chain Rule）

「料理が辛い原因」を調べるとき、調味料A → ソースB → 完成品C と段階がある場合、各段階の影響を掛け算してたどります。

数学では **連鎖律** と呼ばれます。

```
∂Loss/∂a = ∂Loss/∂c × ∂c/∂b × ∂b/∂a
```

各 `×` の右側がローカル勾配（前節で記録したもの）です。

### 3.2 `backward()` のコード

`src/microgpt.js:139` の `backward()` は次の手順で動きます。

```javascript
backward() {
    // 1. 全ノードをトポロジカルソート（依存関係順に並べる）
    const topo = [];
    const visited = new Set();
    const buildTopo = (v) => {
        if (!visited.has(v)) {
            visited.add(v);
            for (const child of v._children) buildTopo(child);
            topo.push(v);
        }
    };
    buildTopo(this);

    // 2. 自分自身（損失）の勾配を 1 に設定
    this.grad = 1;  // ∂Loss/∂Loss = 1（起点）

    // 3. 逆順にたどり、連鎖律を適用
    for (let i = topo.length - 1; i >= 0; i--) {
        const v = topo[i];
        for (let j = 0; j < v._children.length; j++) {
            const child = v._children[j];
            const localGrad = v._localGrads[j];
            child.grad += localGrad * v.grad;
            //           └─ ローカル勾配 × 上流からの勾配 = 連鎖律
        }
    }
}
```

#### 処理の流れを図で表すと

```
損失 (grad = 1.0)    ← 起点
  │
  ↓ ローカル勾配 × 1.0
中間ノードA (grad = 0.5)
  │
  ↓ ローカル勾配 × 0.5
パラメータX (grad = 0.15)  ← 「Xを変えると損失が0.15変わる」
```

### 3.3 トポロジカルソートの役割

計算グラフには分岐や合流があります。トポロジカルソートにより、あるノードの勾配を計算する前に、そのノードに流入する上流の勾配が全て計算済みであることが保証されます。

### 3.4 `grad +=` が `=` ではない理由

同じノードが複数の計算パスで使われる場合（例: 同じ重みが複数の位置で参照される）、各パスからの勾配を **足し合わせる** 必要があります。これは多変数の連鎖律に基づくもので、`=` だと片方の影響が消えてしまいます。

---

## 4. バックプロパゲーションとGPTの各パーツの関係

ここからが本題です。GPTの各コンポーネントが「学習できる」のは、すべてバックプロパゲーションのおかげです。

### 4.1 全体像

```
フォワードパス（前方計算）          バックプロパゲーション（逆伝播）
────────────────────           ────────────────────

Embedding（埋め込み）               ← 勾配がここまで届く
    ↓                                  ↑
RMSNorm（正規化）                   ← 勾配が通過
    ↓                                  ↑
Attention（注意機構）               ← Q,K,V,Woの重みに勾配が届く
    ↓ + 残差接続                       ↑ 残差接続で勾配がバイパス
RMSNorm                             ← 勾配が通過
    ↓                                  ↑
MLP（全結合層）                     ← fc1,fc2の重みに勾配が届く
    ↓ + 残差接続                       ↑ 残差接続で勾配がバイパス
lm_head（出力層）                   ← 重みに勾配が届く
    ↓                                  ↑
softmax → 交差エントロピー損失     ← ここが起点（grad = 1）
```

重要な点: **フォワードパスの逆順** で勾配が流れます。

### 4.2 Embedding（埋め込み）と勾配

**Embeddingとは:** 各文字を16次元の数値ベクトルに変換するルックアップテーブルです（`src/microgpt.js:237-239`）。

```javascript
const tokEmb = stateDict.wte[tokenId];  // トークン埋め込み
const posEmb = stateDict.wpe[posId];    // 位置埋め込み
let x = tokEmb.map((t, i) => t.add(posEmb[i]));  // 合成
```

`wte` と `wpe` は `Value` の2次元配列（`src/microgpt.js:347-348`）なので、`add()` がグラフに記録されます。

**勾配の意味:** 学習中にバックプロパゲーションが実行されると、`wte[tokenId]` の各次元に勾配が設定されます。
例えば `wte[3][5].grad = -0.02` なら、「文字ID=3の埋め込みの5番目の次元を少し増やすと損失が減る」という意味です。

**直感的な理解:** 最初は全ての文字がランダムなベクトルですが、学習が進むと似た使われ方をする文字（例: 母音同士）のベクトルが近づきます。これは勾配が「この文字のベクトルをこの方向に動かせ」と毎ステップ指示し続けた結果です。可視化パネル「埋め込み空間（PCA 2D）」でこの変化を確認できます。

### 4.3 Attention（注意機構）と勾配

Attentionは「文脈のどこに注目するか」を学習する仕組みです（`src/microgpt.js:249-299`）。

#### 4.3.1 Q/K/V 射影の学習

```javascript
const q = linear(x, stateDict[`layer${li}.attn_wq`]);  // Query
const k = linear(x, stateDict[`layer${li}.attn_wk`]);  // Key
const v = linear(x, stateDict[`layer${li}.attn_wv`]);  // Value
```

`linear()` は `Value.mul()` と `Value.add()` の組み合わせ（`src/microgpt.js:199-203`）なので、重み行列 `attn_wq`, `attn_wk`, `attn_wv` への勾配が自動で計算されます。

**勾配の意味:**
- `attn_wq` の勾配 → 「何を探すか」の変換方法を調整
- `attn_wk` の勾配 → 「何を持っているか」の表現方法を調整
- `attn_wv` の勾配 → 「参照された時に渡す情報」の変換方法を調整

#### 4.3.2 Attention重みを通じた勾配

```javascript
// 内積スコア
const attnLogits = kH.map(kHt => {
    return qH.reduce((sum, qHj, j) => sum.add(qHj.mul(kHt[j])), new Value(0))
        .div(Math.sqrt(HEAD_DIM));
});
// softmaxで確率化
const attnWeights = softmax(attnLogits);
```

ここも全て `Value` 演算です。逆伝播すると、勾配は softmax → 内積 → Q, K へと逆流します。

つまり「この位置にもっと注目すべきだった」「この位置は無視すべきだった」という情報が、Q/K の重み行列まで伝わるのです。可視化パネル「Attention重み（ヘッド別）」で、学習が進むにつれ注目パターンが変化するのはこの結果です。

#### 4.3.3 Multi-Head: ヘッドごとの専門化

4つのヘッドは独立に勾配を受け取ります。あるヘッドが「母音の後に来る子音」に注目するパターンを学び、別のヘッドが「名前の先頭文字」に注目するパターンを学ぶ ── こうした専門化は、各ヘッドに異なる勾配が流れることで自然に起きます。

### 4.4 MLP（全結合層）と勾配

MLPはAttentionで集めた情報を非線形に変換する層です（`src/microgpt.js:302-312`）。

```javascript
x = linear(x, stateDict[`layer${li}.mlp_fc1`]);  // 16次元 → 64次元（拡大）
x = x.map(xi => xi.relu());                       // ReLU活性化（非線形）
x = linear(x, stateDict[`layer${li}.mlp_fc2`]);  // 64次元 → 16次元（縮小）
```

#### ReLUと勾配の「ゲート」

`relu()` は「負の値を0にする」関数ですが、勾配の視点では **ゲート（門）** として働きます。

```javascript
relu() {
    return new Value(Math.max(0, this.data), [this], [this.data > 0 ? 1 : 0]);
}
```

- `data > 0` → ローカル勾配 = 1（勾配がそのまま通過）
- `data ≤ 0` → ローカル勾配 = 0（勾配が遮断される）

つまり、64個のニューロンのうち活性化している（値が正の）ものだけが勾配を受け取り、学習に参加します。
可視化パネル「MLPニューロン活性化」の黒い部分は勾配が0で、学習されていないニューロンです。

**直感:** MLPの64ニューロンのうち、ある入力に対して反応するニューロンだけが更新される。入力パターンによって更新されるニューロンが異なるため、パターンごとの「専門家」が自然と育つ仕組みです。

### 4.5 残差接続 — 勾配の高速道路

残差接続（`src/microgpt.js:299, 311`）は学習を安定させる重要な構造です。

```javascript
x = x.map((a, i) => a.add(xResidual[i]));  // x = 変換結果 + 元の入力
```

**勾配の視点:** `add()` のローカル勾配は `[1, 1]` です。つまり上流からの勾配が **そのまま** `xResidual`（元の入力）にも伝わります。

```
勾配の流れ:
                    ┌───────────────────┐
                    │  Attention / MLP  │
                    └────────┬──────────┘
                             │ (勾配が減衰する可能性)
       ┌────────────────────┤
       │                     │
       ↓ (勾配 × 1.0)       ↓ (勾配 × ローカル勾配)
    残差経路              変換経路
    （直通）            （Attention/MLP通過）
```

残差接続がなければ、深い層のパラメータほど勾配が小さくなり（**勾配消失**）、学習が困難になります。残差接続は「勾配のバイパス」として、奥の層まで勾配を確実に届けます。

可視化パネル「残差ストリームの変遷」で、各ステージでL2ノルムが急激に変化しないのは、残差接続による安定化の効果です。

### 4.6 RMSNorm — 勾配の安定化

正規化（`src/microgpt.js:217-221`）は、ベクトルのスケールを揃えます。

```javascript
function rmsnorm(x) {
    const ms = x.reduce((sum, xi) => sum.add(xi.mul(xi)), new Value(0)).div(x.length);
    const scale = ms.add(1e-5).pow(-0.5);
    return x.map(xi => xi.mul(scale));
}
```

**勾配の視点:** 正規化がないと、層を通るたびに値のスケールが大きくなったり小さくなったりし、勾配も不安定になります。RMSNormは値のスケールを一定に保つことで、勾配のスケールも安定させます。

### 4.7 lm_head（出力層）と損失

最終的に、隠れ状態を語彙サイズのスコア（logits）に変換し、損失を計算します（`src/microgpt.js:316, 412-414`）。

```javascript
// フォワードパス最後: 隠れ状態 → logits
const logits = linear(x, stateDict.lm_head);

// 学習ループ内: logits → 確率 → 損失
const probs = softmax(logits);
const lossT = probs[targetId].log().neg();  // -log(正解の確率)
```

**損失がバックプロパゲーションの起点になる理由:**
- 正解トークンの確率が高い → `-log(高い確率)` → 損失が小さい → 良い予測
- 正解トークンの確率が低い → `-log(低い確率)` → 損失が大きい → 悪い予測

`loss.backward()` を呼ぶと、この損失ノードから計算グラフを逆にたどり、全ての `Value`（全てのパラメータ）に勾配が設定されます。

---

## 5. バックプロパゲーション後 — Adamによるパラメータ更新

勾配が計算されたら、それを使ってパラメータを更新します（`src/microgpt.js:441-455`）。

### 5.1 なぜ単純な引き算ではダメなのか

最も素朴な更新は `p.data -= 学習率 * p.grad` です。しかしこれには問題があります。

- 勾配がノイズで振動する → 学習が不安定
- パラメータによって勾配の大きさが全く違う → 同じ学習率では効率が悪い

### 5.2 Adamの仕組み

Adam（`src/microgpt.js:373-382, 444-454`）は、勾配の **移動平均** と **分散** を追跡して、パラメータごとに適応的な更新を行います。

```javascript
// モーメントの更新
m[i] = beta1 * m[i] + (1 - beta1) * p.grad;       // 勾配の移動平均（方向）
v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2;   // 勾配二乗の移動平均（振れ幅）

// バイアス補正（学習初期の過小評価を修正）
const mHat = m[i] / (1 - Math.pow(beta1, step + 1));
const vHat = v[i] / (1 - Math.pow(beta2, step + 1));

// パラメータ更新
p.data -= lrT * mHat / (Math.sqrt(vHat) + epsAdam);
```

**直感:**
- `m`（1次モーメント）: 「最近の勾配はどの方向を指しているか」の平均。ノイズを平滑化
- `v`（2次モーメント）: 「勾配がどのくらい振れるか」の指標。振れが大きいパラメータは慎重に更新
- 割り算 `mHat / √vHat`: 振れが大きいパラメータほど更新幅を自動で小さくする

### 5.3 勾配のリセット

```javascript
p.grad = 0;  // 勾配をリセット（次ステップに備える）
```

`backward()` は `grad +=` で蓄積するため、リセットしないと前のステップの勾配が残ってしまいます。

---

## 6. 学習1ステップの全体フロー

`src/microgpt.js:390-483` の1ステップをまとめます。

```
1. 名前を1つ選ぶ（例: "anna"）
     tokens = [BOS, a, n, n, a, BOS]

2. 各位置でフォワードパス
     位置0: BOS → gpt() → "次は a" の確率を予測
     位置1: a   → gpt() → "次は n" の確率を予測
     ...各位置で損失を計算

3. 全位置の損失を平均
     loss = (loss_0 + loss_1 + ... + loss_n) / n

4. バックプロパゲーション
     loss.backward()
     → 計算グラフを逆にたどり、全パラメータの .grad を計算
     → wte, wpe, attn_wq, attn_wk, attn_wv, attn_wo,
        mlp_fc1, mlp_fc2, lm_head の全要素に勾配が届く

5. Adamでパラメータ更新
     各パラメータ: p.data -= lr * (補正済み勾配)
     各パラメータ: p.grad = 0  （リセット）

6. 次のステップへ（1に戻る）
```

---

## 7. 勾配フロー可視化パネルの読み方

可視化パネル「勾配フロー（L2ノルム）」（`src/viz/gradientFlow.js`）は、パラメータグループごとの勾配の大きさを棒グラフで表示します。

```javascript
// src/microgpt.js:430-439 で計算
for (const [name, mat] of Object.entries(stateDict)) {
    let sumSq = 0;
    for (const row of mat) {
        for (const p of row) {
            sumSq += p.grad * p.grad;
        }
    }
    gradNorms[name] = Math.sqrt(sumSq);  // L2ノルム = 勾配の「大きさ」
}
```

### 各バーの見方

| バーのラベル | パラメータ | 意味 |
|---|---|---|
| `wte` | トークン埋め込み | 文字ベクトルの更新量 |
| `wpe` | 位置埋め込み | 位置情報の更新量 |
| `L0.wq` | Layer 0 の Query重み | 「何を探すか」の学習度合い |
| `L0.wk` | Layer 0 の Key重み | 「何を持っているか」の学習度合い |
| `L0.wv` | Layer 0 の Value重み | 「参照時に渡す情報」の学習度合い |
| `L0.wo` | Layer 0 の出力射影 | ヘッド統合の学習度合い |
| `L0.fc1` | Layer 0 の MLP第1層 | 非線形変換（拡大）の学習度合い |
| `L0.fc2` | Layer 0 の MLP第2層 | 非線形変換（縮小）の学習度合い |
| `head` | lm_head | 最終出力層の更新量 |

### 健全な学習の目安

- **全バーが適度な大きさ**: 全パラメータがバランスよく学習されている
- **特定のバーだけ極端に大きい**: 勾配爆発の兆候 → 学習率が高すぎる可能性
- **特定のバーがほぼ0**: 勾配消失 → そのパラメータが学習に参加していない
- **学習が進むにつれ全体的に小さくなる**: 正常。収束に近づいている

---

## 8. よくある疑問

### Q. なぜ `Value` でスカラー演算なのか？テンソルのほうが速いのでは？

はい。PyTorch等の実用フレームワークはテンソル（行列）単位で自動微分します。
`microgpt` はスカラー単位にすることで、**勾配が1つの数値から別の数値にどう伝播するか** を追跡可能にしています。学習のための実装ではなく、仕組みを理解するための実装です。

### Q. バックプロパゲーションは毎ステップ全パラメータの勾配を計算するのか？

はい。`loss.backward()` 1回で、計算グラフに含まれる全ての `Value` ノードに勾配が計算されます。`microgpt` では約10,000個のパラメータ全てに勾配が設定されます。

### Q. Attention の重みも勾配で学習されるのか？

はい。Attention の重み（softmax の出力）自体は直接のパラメータではありませんが、それを生成する `attn_wq`, `attn_wk`, `attn_wv` が学習パラメータです。勾配は softmax → 内積 → Q/K/V射影行列 と逆流し、「どのような注目パターンが損失を下げるか」を学習します。

### Q. 残差接続を外したらどうなるか？

勾配消失が起きやすくなり、特に `N_LAYER` を増やした場合に学習が困難になります。
残差接続は勾配の大きさを保つ「ショートカット」であり、深いネットワークの学習を可能にする重要な構造です。

---

## 9. コードで追体験する

実際に計算グラフと勾配の動きを確認するには、ブラウザのコンソールで次を試してください。

```javascript
// 1. 小さな計算グラフを作る
const a = new Value(2.0);
const b = new Value(3.0);
const c = a.mul(b);     // c = 6.0
const d = c.add(a);     // d = 6.0 + 2.0 = 8.0

// 2. 逆伝播
d.backward();

// 3. 勾配を確認
console.log(a.grad);  // 4.0  ← aはmulとaddの両方で使われるので 3 + 1 = 4
console.log(b.grad);  // 2.0  ← ∂d/∂b = ∂(a*b+a)/∂b = a = 2
console.log(c.grad);  // 1.0  ← ∂d/∂c = 1（addのローカル勾配）
```

`a.grad = 4.0` は「`a` を 0.001 増やすと `d` が 0.004 増える」ことを意味します。

---

## 10. まとめ

| 概念 | 役割 | コード上の場所 |
|------|------|--------------|
| `Value` クラス | 計算の履歴を記録する「賢い数値」 | `src/microgpt.js:75` |
| ローカル勾配 | 各演算の変化率 | 各演算メソッド内の `localGrads` |
| `backward()` | 連鎖律で全ノードの勾配を逆算 | `src/microgpt.js:139` |
| 勾配 (`grad`) | 「この値を変えたら損失がどれだけ変わるか」 | 各 `Value` の `.grad` プロパティ |
| Adam | 勾配を使ってパラメータを賢く更新 | `src/microgpt.js:444` |

バックプロパゲーションは「魔法」ではなく、**演算ごとの変化率を記録して、出力から入力へ掛け算で伝播させる** だけです。この仕組みのおかげで、Embedding、Attention、MLP、lm_head ── GPTの全パーツが同時に、自動で、正しい方向に学習できます。
